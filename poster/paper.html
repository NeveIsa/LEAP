<!DOCTYPE html><script src="https://cdn.jsdelivr.net/npm/texme@1.2.2"></script>
<style>
body main{
    background-color: gainsboro;
}
.diagram-iframe {
    width: 100%;
    height: 600px;
    border: 1px solid #ddd;
    border-radius: 8px;
    margin: 20px 0;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
.diagram-iframe.tall {
    height: 750px;
}
</style>
<textarea>

# Interactive Classroom RPC for Numerical Methods:
## A Scalable Framework for Live Teaching, Real-Time Analytics, and Adaptive Assessment

### Abstract
Teaching numerical methods and optimization benefits from experiential learning rather than passive derivation, yet in large classes it remains challenging to capture student activity, provide immediate feedback, and maintain classroom synchronization. We present an **interactive classroom framework** that exposes instructor-defined Python functions as HTTP endpoints (FastAPI), enables students to invoke them from any client environment, and persistently logs every interaction to DuckDB for real-time analytics and longitudinal analysis. 

Instructors organize content as modular "experiments" containing callable functions (e.g., bisection search, Newton's method, gradient descent) paired with interactive dashboards for visualization. Students engage through lightweight clients—Python scripts, Jupyter notebooks, or command-line interfaces—to explore algorithmic behavior dynamically. The framework further supports **live quizzes** to enhance classroom participation, featuring Markdown-authored assessments with LaTeX mathematical notation and syntax-highlighted code blocks, all seamlessly integrated with the unified logging infrastructure for immediate statistical feedback.

We present the system architecture, instructor authoring workflow, and pedagogical deployment patterns including live demonstrations, real-time polling, and formative assessment. Our evaluation framework combines engagement metrics derived from interaction logs with pre/post conceptual assessments. By open-sourcing this tool, we aim to democratize access to scalable, auditable active learning environments for computational education.

---

## 1. Introduction and Motivation
Active learning methodologies have demonstrated significant efficacy in computational education, yet conventional classroom environments predominantly rely on static presentations, whiteboard demonstrations, or isolated computing environments. These traditional approaches constrain instructors' capacity to monitor student engagement, capture learning processes, and deliver immediate, personalized feedback at scale.

This work addresses these pedagogical limitations through a distributed, lightweight classroom system that transforms instructor-authored Python functions into publicly accessible HTTP endpoints. By maintaining comprehensive interaction logs, the framework enables both real-time monitoring and retrospective analysis of student learning patterns, algorithmic exploration strategies, and conceptual development trajectories.

---

## 2. System Architecture and Implementation
**Core Framework.** The system employs a FastAPI-based microservice architecture with RESTful endpoints supporting:
- `/functions` → enumeration of available computational activities
- `/call` → synchronous invocation of instructor-defined methods
- `/logs` & `/experiments` → metadata retrieval and monitoring interfaces  
- `/health` → deployment status and system diagnostics

**Modular Experiment Design.** Each pedagogical unit is encapsulated as a self-contained experiment directory featuring:
- `funcs/`: Python modules exposing instructor-authored functions
- `ui/`: interactive visualization dashboards and custom interfaces
- `quiz/`: assessment materials authored in extended Markdown with LaTeX support
- `db/`: isolated DuckDB instance for experiment-specific data persistence

**Comprehensive Data Capture.** All student interactions are atomically logged to DuckDB, preserving function arguments, return values, execution timestamps, error traces, and session metadata. This granular logging infrastructure supports both individual learning analytics and aggregate classroom insights.

**Security and Isolation.** Administrative access utilizes PBKDF2-based credential hashing with configurable iteration counts. Experiment isolation prevents data leakage between concurrent classroom sessions.

**Figure 1: System Architecture**

<iframe src="architecture-diagram.html" class="diagram-iframe" title="System Architecture Diagram"></iframe>

*(Figure 1: System architecture showing the distributed client-server model with persistent logging layer.)*

**Performance and Scalability.** The lightweight architecture supports concurrent multi-user sessions while maintaining low latency response times. DuckDB's columnar storage provides efficient analytics queries even with extensive interaction histories.

**Deployment Flexibility.** The system can be deployed locally for individual courses, on institutional servers for department-wide use, or in cloud environments for large-scale implementations. Docker containerization ensures consistent deployment across different platforms.

---

## 3. Pedagogical Applications and Classroom Integration
**Curricular Deployment.** Instructors develop experiment modules targeting fundamental computational topics:
- **Numerical root-finding:** Interactive comparison of bisection, Newton-Raphson, and secant methods with convergence visualization
- **Optimization algorithms:** Hands-on exploration of gradient descent variants, proximal operators, and constraint handling
- **Applied computational problems:** Parameter estimation in regression models, clustering algorithm behavior, and resource allocation optimization

**Multi-Modal Student Engagement.** Students interact through diverse client environments—Python scripts, Jupyter notebooks, command-line interfaces, or web browsers—enabling seamless integration with existing computational workflows. The comprehensive logging infrastructure enables instructors to replay student exploration sessions, generate algorithmic performance leaderboards, and identify systematic conceptual difficulties.

**Integrated Assessment Framework.** The quiz subsystem supports rich multimedia assessments authored in Markdown with embedded LaTeX mathematical expressions and syntax-highlighted code segments. Assessment responses flow through the unified `/call` API, enabling real-time participation analytics and immediate statistical feedback during live classroom sessions.

**Figure 2: User Interaction Flow**

<iframe src="user-flow-diagram.html" class="diagram-iframe tall" title="User Interaction Flow Diagram"></iframe>

---

## 4. Evaluation Framework and Preliminary Results
**Pilot Deployment Outcomes.** Initial classroom implementations demonstrated measurable improvements in student engagement patterns:
- **Algorithmic Exploration:** Students conducted systematic comparisons between numerical methods (Newton-Raphson vs. bisection search) with quantifiable parameter variations
- **Real-Time Participation:** Live polling and formative assessment occurred seamlessly without traditional LMS infrastructure overhead
- **Dynamic Visualization:** Immediate visual feedback enabled students to observe algorithmic convergence behavior and parameter sensitivity in real-time

**Comprehensive Evaluation Methodology.**
- **Engagement Analytics:** Quantitative metrics derived from interaction logs including invocation frequency, algorithmic diversity exploration, and session duration patterns
- **Learning Assessment:** Pre/post conceptual evaluations measuring numerical methods understanding and computational thinking development
- **Instructor Experience Studies:** Usability evaluation focusing on deployment complexity, content authoring workflows, and classroom integration effectiveness

**Research Directions and System Evolution.**
- **Enhanced Visualization:** Advanced dashboard components including algorithm convergence traces, multi-dimensional parameter space exploration, and interactive contour plotting
- **Intelligent Analytics:** Machine learning-driven insights for instructors including error pattern clustering, learning trajectory analysis, and predictive student support recommendations
- **Ecosystem Integration:** Seamless LMS connectivity while preserving the framework's lightweight, standalone deployment characteristics

---

## 5. Conclusion and Impact
This work presents a novel distributed framework for computational education that addresses fundamental scalability and engagement challenges in numerical methods instruction. By exposing instructor-authored functions as accessible HTTP endpoints with comprehensive interaction logging, the system enables unprecedented visibility into student learning processes while maintaining pedagogical flexibility and deployment simplicity.

The integration of real-time assessment capabilities, sophisticated analytics infrastructure, and multi-modal client support creates a unified platform for active learning that scales from small seminars to large lecture courses. Our open-source approach democratizes access to advanced educational technology, reducing institutional barriers to adopting evidence-based computational pedagogy.

Through rigorous evaluation combining interaction analytics with learning outcome assessment, this framework contributes both a practical tool for educators and empirical insights into how students engage with computational problem-solving in interactive environments.

---

## References
- Freeman, S., Eddy, S. L., McDonough, M., Smith, M. K., Okoroafor, N., Jordt, H., & Wenderoth, M. P. (2014). Active learning increases student performance in science, engineering, and mathematics. *Proceedings of the National Academy of Sciences*, 111(23), 8410-8415.
- Prince, M. (2004). Does active learning work? A review of the research. *Journal of Engineering Education*, 93(3), 223-231.
- Mazur, E. (1997). *Peer instruction: A user's manual*. Prentice Hall.
- Denny, P., Hamer, J., Luxton-Reilly, A., & Purchase, H. (2008). PeerWise: Students sharing their multiple choice questions. *ACM SIGCSE Bulletin*, 40(3), 282-286.
- **Source Code:** https://github.com/NeveIsa/TeachingOptimization
- **Demo Environment:** [Live deployment URL when available]
