<!DOCTYPE html><script src="https://cdn.jsdelivr.net/npm/texme@1.2.2"></script>
<style>
body main{
    background-color: white;
}
.diagram-iframe {
    width: 100%;
    height: 600px;
    border: 1px solid #ddd;
    border-radius: 8px;
    margin: 20px 0;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
.diagram-iframe.tall {
    height: 750px;
}
.diagram-iframe.compact {
    height: 500px;
}
</style>
<textarea>

# Interactive Computational Learning Platform:
## A Unified Framework for Algorithm Exploration, Real-Time Analytics, and Live Assessment

### Abstract
Interactive computational exercises can help students explore algorithmic concepts through direct labation. We present a platform that exposes Python functions as HTTP endpoints, allowing students to call functions remotely while logging all interactions for later analysis. 

The platform organizes content as modular "labs" containing callable Python functions. Current implementation includes 9 labs with simple functions for educational exploration: basic mathematical functions (square, cubic, quadratic), optimization functions (Rosenbrock), graph traversal helpers, Monte Carlo sampling, differential equation examples, and matrix operations. Students call these functions via HTTP from any client while all interactions are logged to DuckDB. The system includes unified authentication and a quiz framework for one lab.

We describe the system architecture and current implementation. The platform is available as open-source software and designed to support educational research through interaction logging.

---

## 1. Introduction and Motivation
Active learning methodologies have demonstrated significant efficacy in computational education, yet conventional classroom environments predominantly rely on static presentations, whiteboard demonstrations, or isolated computing environments. These traditional approaches constrain instructors' capacity to monitor student engagement, capture learning processes, and deliver immediate, personalized feedback at scale.

This work presents a platform that exposes Python functions as HTTP endpoints for educational use. The current implementation includes basic functions across several areas: mathematical operations, simple optimization problems, graph traversal helpers, Monte Carlo sampling, and basic examples for differential equations. All function calls are logged for analysis.

---

## 2. System Architecture and Implementation
**2.1 Core Infrastructure**

The platform uses a RESTful architecture built on FastAPI, providing access to computational functions:

- **`/functions`** — Dynamic function discovery and signature introspection
- **`/call`** — Synchronous function execution with comprehensive error handling
- **`/logs`** — Flexible querying interface supporting temporal and contextual filtering
- **`/labs`** — Session management and lab lifecycle control
- **`/admin`** — Authentication and administrative operations

**2.2 Modular Lab Architecture**

Pedagogical content follows a **self-contained module design** enabling independent deployment:

```
labs/<module>/
├── funcs/          # Instructor-authored computational functions
├── ui/             # Interactive dashboards and visualizations  
├── quiz/           # Assessment materials (Markdown + LaTeX)
└── db/             # Isolated DuckDB analytics store
```

This design allows multiple labs to run while sharing a common API.

**2.3 Analytics Infrastructure**

Every student interaction generates a **structured log entry** preserving:
- Function invocation details (name, arguments, timestamp)
- Execution results or comprehensive error traces
- Session context (student ID, lab, trial identification)
- Performance metrics (execution time, resource utilization)

DuckDB's **columnar storage engine** enables efficient analytics queries over extensive interaction datasets, supporting real-time dashboards and longitudinal analysis.

**2.4 Security and Authentication**

The platform uses unified authentication across all labs:

- **PBKDF2 credential hashing** (240,000 iterations, SHA-256)
- **Session-based access control** with configurable timeouts
- **Global authentication scope** enabling seamless cross-lab navigation
- **Data isolation** maintaining lab-level privacy boundaries

**Figure 1: System Architecture**

<iframe src="architecture-diagram-concise.html" class="diagram-iframe compact" title="System Architecture Diagram" style="height: 400px;"></iframe>

*(Figure 1: System architecture showing the distributed client-server model with persistent logging layer.)*

**Performance and Scalability.** The lightweight architecture supports concurrent multi-user sessions while maintaining low latency response times. DuckDB's columnar storage provides efficient analytics queries even with extensive interaction histories.

**Deployment Flexibility.** The system can be deployed locally for individual courses, on institutional servers for department-wide use, or in cloud environments for large-scale implementations. Docker containerization ensures consistent deployment across different platforms.

---

## 3. Computational Domains and Pedagogical Applications

**3.1 Current Function Library**

The platform includes simple computational functions across several labs:

- **Basic mathematics:** Square, cubic, and quadratic functions for algorithm exploration
- **Optimization:** Rosenbrock function for testing optimization approaches  
- **Graph algorithms:** Neighbor-finding functions for 5x5 grid traversal
- **Monte Carlo:** Random point generation and circle inclusion testing for pi estimation
- **Differential equations:** Simple derivative function for Euler method exploration
- **Linear algebra:** Basic matrix multiplication function

**3.2 Educational Interaction Model**

Students interact with functions through HTTP calls:

- **Function calls:** Students invoke functions with different parameters to explore behavior
- **Logging:** All function calls, parameters, results, and errors are recorded
- **Multiple clients:** Students can use Python scripts, command-line tools, or potentially Jupyter notebooks
- **Real-time feedback:** Immediate function results enable iterative exploration

**3.3 Assessment and Monitoring**

- **Quiz system:** One lab (quizlab) includes Markdown-based quizzes with LaTeX math support
- **Interaction logs:** Comprehensive logging enables instructors to review student exploration patterns
- **Dashboard UI:** Web interfaces provide access to functions and basic monitoring

**3.4 Technical Infrastructure**

- **Data storage:** DuckDB database for interaction logging and analytics
- **Web framework:** FastAPI-based server with RESTful endpoints
- **Authentication:** Unified login system across all labs
- **Lab isolation:** Each lab has its own function namespace and data

**3.5 Multi-Modal Client Architecture**

The platform's client-agnostic design supports diverse interaction paradigms:

- **Python scripts:** Direct API integration enabling programmatic algorithmic exploration
- **Jupyter notebooks:** Rich computational narratives with embedded function calls and visualization
- **Command-line interfaces:** Lightweight interaction for focused algorithmic investigation
- **Web dashboards:** Browser-based exploration with real-time visualization components

This flexibility allows students to engage through their preferred computational environment while ensuring consistent data capture and analytics across all interaction modes.

**Figure 2: User Interaction Flow**

<iframe src="user-flow-diagram-concise.html" class="diagram-iframe tall" title="User Interaction Flow Diagram" style="height: 800px;"></iframe>

---

## 4. Evaluation Framework and Empirical Results

**4.1 Planned Evaluation Methodology**

We propose a mixed-methods evaluation framework to assess the platform's educational effectiveness across diverse computational curricula:

**4.2 Student Engagement Analysis**

Planned metrics for analyzing interaction logs include:

- **Exploration patterns:** Function call frequency and parameter variation strategies
- **Cross-domain usage:** Student transitions between different algorithmic labs
- **Session duration:** Time spent on algorithmic exploration tasks
- **Error patterns:** Analysis of mistakes and correction strategies

**4.3 Learning Outcomes Assessment**

Proposed pre/post evaluation using computational thinking assessments:

- **Problem decomposition:** Ability to break down algorithmic problems
- **Behavioral prediction:** Skills in anticipating algorithm performance
- **Knowledge transfer:** Application of concepts across different algorithmic domains

**4.4 Instructor Experience Evaluation**

Planned instructor feedback collection on:

- **Setup time:** Deployment efficiency compared to traditional lab configuration
- **Content creation:** Ease of developing new computational labs
- **Classroom monitoring:** Visibility into student learning during live sessions

**4.5 Implementation and Evaluation Timeline**

Current platform status and planned evaluation:

- **Technical foundation:** Core platform with 9 computational domains implemented
- **Deployment readiness:** System ready for classroom trials in upcoming academic term
- **Data collection framework:** Interaction logging and analytics infrastructure complete
- **Future development:** Additional computational domains and analytics features planned based on evaluation results

---

## 5. Conclusions and Broader Impact

**5.1 Technical Contributions**

This work presents a simple platform for exposing computational functions to students via HTTP. Key features include:

- **Unified authentication architecture** reducing cognitive load while maintaining security isolation
- **Language-agnostic API design** enabling diverse client ecosystem integration
- **Real-time analytics infrastructure** providing immediate instructional feedback at scale
- **Modular lab architecture** supporting independent content development and deployment
- **Standard lab templates** enabling instructors to rapidly create new labs and labs through drag-and-drop functionality

**5.2 Pedagogical Impact**

The platform is designed to support evaluation across 9 computational domains. The interaction logging capability will enable analysis of student engagement patterns and algorithmic learning processes that are not easily visible in traditional assessment approaches.

**5.3 Open Source Implementation**

The platform is available as open-source software. The system uses standard web technologies (FastAPI, DuckDB) and can be deployed locally or on institutional servers.

**5.4 Research Implications**

The comprehensive interaction logging infrastructure creates new opportunities for computational education research. Fine-grained behavioral data enables investigation of learning transfer between algorithmic domains, identification of common misconception patterns, and development of adaptive instructional interventions.

**Platform Status:** Current implementation includes 9 labs with basic functions: mathematical operations (square, cubic, quadratic), the Rosenbrock function, graph grid navigation helpers, Monte Carlo sampling functions, simple differential equation examples, and basic matrix operations. The system includes unified authentication and logging infrastructure.

---

## References

**Pedagogical Foundations:**
- Freeman, S., et al. (2014). Active learning increases student performance in science, engineering, and mathematics. *Proceedings of the National Academy of Sciences*, 111(23), 8410-8415.
- Prince, M. (2004). Does active learning work? A review of the research. *Journal of Engineering Education*, 93(3), 223-231.
- Mazur, E. (1997). *Peer instruction: A user's manual*. Prentice Hall.

**Educational Technology:**
- Denny, P., et al. (2008). PeerWise: Students sharing their multiple choice questions. *ACM SIGCSE Bulletin*, 40(3), 282-286.
- Ihantola, P., et al. (2010). Review of recent systems for automatic assessment of programming assignments. *ACM Computing Surveys*, 43(3), 1-35.

**Computational Thinking:**
- Wing, J. M. (2006). Computational thinking. *Communications of the ACM*, 49(3), 33-35.
- Grover, S., & Pea, R. (2013). Computational thinking in K–12: A review of the state of the field. *Educational Researcher*, 42(1), 38-43.

**Platform Resources:**
- **Source Code:** https://github.com/NeveIsa/TeachingOptimization
- **Documentation:** [Platform deployment and authoring guides]
- **Demo Environment:** [Live institutional deployment when available]
